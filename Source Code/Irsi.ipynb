{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "158f47a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Weighted KNN: 93.33%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data for better distance computation\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the Weighted KNN function\n",
    "def weighted_knn(X_train, y_train, X_test, k=5):\n",
    "    predictions = []\n",
    "    \n",
    "    for test_point in X_test:\n",
    "        # Compute distances between the test point and all training points\n",
    "        distances = np.linalg.norm(X_train - test_point, axis=1)\n",
    "        \n",
    "        # Get the indices of the k nearest neighbors\n",
    "        nearest_indices = np.argsort(distances)[:k]\n",
    "        \n",
    "        # Retrieve the classes of the nearest neighbors\n",
    "        nearest_classes = y_train[nearest_indices]\n",
    "        \n",
    "        # Compute weights (inverse of distance, avoid division by zero)\n",
    "        weights = 1 / (distances[nearest_indices] + 1e-5)\n",
    "        \n",
    "        # Aggregate weights for each class\n",
    "        class_weights = {}\n",
    "        for i, cls in enumerate(nearest_classes):\n",
    "            class_weights[cls] = class_weights.get(cls, 0) + weights[i]\n",
    "        \n",
    "        # Predict the class with the highest total weight\n",
    "        predicted_class = max(class_weights, key=class_weights.get)\n",
    "        predictions.append(predicted_class)\n",
    "    \n",
    "    return np.array(predictions)\n",
    "\n",
    "# Apply WKNN\n",
    "k = 5\n",
    "y_pred = weighted_knn(X_train, y_train, X_test, k=k)\n",
    "\n",
    "# Evaluate the results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Weighted KNN: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b32f9f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 105\n",
      "Reduced training size: 15\n",
      "Accuracy with condensed dataset: 97.78%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def knn_predict(X_train, y_train, test_point, k=1):\n",
    "    \"\"\"\n",
    "    Perform KNN classification for a single test point.\n",
    "    \"\"\"\n",
    "    distances = np.linalg.norm(X_train - test_point, axis=1)\n",
    "    nearest_indices = np.argsort(distances)[:k]\n",
    "    nearest_labels = y_train[nearest_indices]\n",
    "    unique, counts = np.unique(nearest_labels, return_counts=True)\n",
    "    return unique[np.argmax(counts)]\n",
    "\n",
    "def condensed_nearest_neighbor(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Condensed Nearest Neighbor algorithm to reduce training dataset size.\n",
    "    \"\"\"\n",
    "    # Start with one randomly selected point\n",
    "    indices = np.random.permutation(len(X_train))\n",
    "    S_X = [X_train[indices[0]]]\n",
    "    S_y = [y_train[indices[0]]]\n",
    "    \n",
    "    for _ in range(10):  # Fixed number of iterations\n",
    "        misclassified = False\n",
    "        for i in indices[1:]:\n",
    "            pred = knn_predict(np.array(S_X), np.array(S_y), X_train[i])\n",
    "            if pred != y_train[i]:\n",
    "                S_X.append(X_train[i])\n",
    "                S_y.append(y_train[i])\n",
    "                misclassified = True\n",
    "        if not misclassified:\n",
    "            break\n",
    "    \n",
    "    return np.array(S_X), np.array(S_y)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply CNN\n",
    "condensed_X_train, condensed_y_train = condensed_nearest_neighbor(X_train, y_train)\n",
    "\n",
    "# Evaluate using KNN\n",
    "y_pred = [knn_predict(condensed_X_train, condensed_y_train, test_point) for test_point in X_test]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Results\n",
    "print(f\"Original training size: {len(X_train)}\")\n",
    "print(f\"Reduced training size: {len(condensed_X_train)}\")\n",
    "print(f\"Accuracy with condensed dataset: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c15640f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 105\n",
      "Reduced training size: 10\n",
      "Accuracy with reduced dataset: 75.56%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def knn_predict(X_train, y_train, test_point, k=1):\n",
    "    \"\"\"\n",
    "    Perform KNN classification for a single test point.\n",
    "    \"\"\"\n",
    "    distances = np.linalg.norm(X_train - test_point, axis=1)\n",
    "    nearest_indices = np.argsort(distances)[:k]\n",
    "    nearest_labels = y_train[nearest_indices]\n",
    "    unique, counts = np.unique(nearest_labels, return_counts=True)\n",
    "    return unique[np.argmax(counts)]\n",
    "\n",
    "def reduced_nearest_neighbor(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Reduced Nearest Neighbor algorithm to reduce training dataset size.\n",
    "    \"\"\"\n",
    "    S_X = list(X_train)\n",
    "    S_y = list(y_train)\n",
    "    \n",
    "    removed = True\n",
    "    while removed:\n",
    "        removed = False\n",
    "        for i in range(len(S_X) - 1, -1, -1):  # Iterate in reverse to avoid indexing issues\n",
    "            temp_X = S_X[:i] + S_X[i+1:]\n",
    "            temp_y = S_y[:i] + S_y[i+1:]\n",
    "            \n",
    "            pred = knn_predict(np.array(temp_X), np.array(temp_y), S_X[i])\n",
    "            if pred == S_y[i]:  # Point can be removed\n",
    "                del S_X[i]\n",
    "                del S_y[i]\n",
    "                removed = True\n",
    "    return np.array(S_X), np.array(S_y)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply RNN\n",
    "reduced_X_train, reduced_y_train = reduced_nearest_neighbor(X_train, y_train)\n",
    "\n",
    "# Evaluate using KNN\n",
    "y_pred = [knn_predict(reduced_X_train, reduced_y_train, test_point) for test_point in X_test]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Results\n",
    "print(f\"Original training size: {len(X_train)}\")\n",
    "print(f\"Reduced training size: {len(reduced_X_train)}\")\n",
    "print(f\"Accuracy with reduced dataset: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea8c3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 105\n",
      "Cleaned training size: 100\n",
      "Accuracy with cleaned dataset: 95.56%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def knn_predict(X_train, y_train, test_point, k=3):\n",
    "    \"\"\"\n",
    "    Perform KNN classification for a single test point.\n",
    "    \"\"\"\n",
    "    distances = np.linalg.norm(X_train - test_point, axis=1)\n",
    "    nearest_indices = np.argsort(distances)[:k]\n",
    "    nearest_labels = y_train[nearest_indices]\n",
    "    unique, counts = np.unique(nearest_labels, return_counts=True)\n",
    "    return unique[np.argmax(counts)]\n",
    "\n",
    "def edited_nearest_neighbor(X_train, y_train, k=3):\n",
    "    \"\"\"\n",
    "    Edited Nearest Neighbor algorithm to clean the dataset.\n",
    "    \"\"\"\n",
    "    S_X, S_y = list(X_train), list(y_train)\n",
    "    \n",
    "    removed = True\n",
    "    while removed:\n",
    "        removed = False\n",
    "        indices_to_remove = []\n",
    "        \n",
    "        for i in range(len(S_X)):\n",
    "            temp_X = S_X[:i] + S_X[i+1:]  # All points except the current one\n",
    "            temp_y = S_y[:i] + S_y[i+1:]\n",
    "            \n",
    "            pred = knn_predict(np.array(temp_X), np.array(temp_y), S_X[i], k=k)\n",
    "            if pred != S_y[i]:  # Misclassified point\n",
    "                indices_to_remove.append(i)\n",
    "        \n",
    "        # Remove all misclassified points at once\n",
    "        for index in sorted(indices_to_remove, reverse=True):\n",
    "            del S_X[index]\n",
    "            del S_y[index]\n",
    "            removed = True\n",
    "    \n",
    "    return np.array(S_X), np.array(S_y)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply ENN\n",
    "k = 3\n",
    "cleaned_X_train, cleaned_y_train = edited_nearest_neighbor(X_train, y_train, k=k)\n",
    "\n",
    "# Evaluate using KNN\n",
    "y_pred = [knn_predict(cleaned_X_train, cleaned_y_train, test_point, k=k) for test_point in X_test]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Results\n",
    "print(f\"Original training size: {len(X_train)}\")\n",
    "print(f\"Cleaned training size: {len(cleaned_X_train)}\")\n",
    "print(f\"Accuracy with cleaned dataset: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249533be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 105\n",
      "Selected training size: 18\n",
      "Accuracy with selected dataset: 88.89%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def knn_predict(X_train, y_train, test_point, k=1):\n",
    "    \"\"\"\n",
    "    Perform KNN classification for a single test point.\n",
    "    \"\"\"\n",
    "    distances = np.linalg.norm(X_train - test_point, axis=1)\n",
    "    nearest_indices = np.argsort(distances)[:k]\n",
    "    nearest_labels = y_train[nearest_indices]\n",
    "    unique, counts = np.unique(nearest_labels, return_counts=True)\n",
    "    return unique[np.argmax(counts)]\n",
    "\n",
    "def selective_nearest_neighbor(X_train, y_train, k=1):\n",
    "    \"\"\"\n",
    "    Selective Nearest Neighbor algorithm to reduce the dataset size.\n",
    "    \"\"\"\n",
    "    S_X, S_y = [], []  # Start with an empty subset\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        if len(S_X) == 0:\n",
    "            S_X.append(X_train[i])\n",
    "            S_y.append(y_train[i])\n",
    "        else:\n",
    "            pred = knn_predict(np.array(S_X), np.array(S_y), X_train[i], k=k)\n",
    "            if pred != y_train[i]:  # Misclassified point\n",
    "                S_X.append(X_train[i])\n",
    "                S_y.append(y_train[i])\n",
    "    \n",
    "    return np.array(S_X), np.array(S_y)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply SNN\n",
    "k = 3\n",
    "selected_X_train, selected_y_train = selective_nearest_neighbor(X_train, y_train, k=k)\n",
    "\n",
    "# Evaluate using KNN\n",
    "y_pred = [knn_predict(selected_X_train, selected_y_train, test_point, k=k) for test_point in X_test]\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Results\n",
    "print(f\"Original training size: {len(X_train)}\")\n",
    "print(f\"Selected training size: {len(selected_X_train)}\")\n",
    "print(f\"Accuracy with selected dataset: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b92859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training size: 105\n",
      "Boundary training size: 9\n",
      "Accuracy with boundary dataset: 44.44%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def find_voronoi_boundary_points(X_train, y_train, k=3):\n",
    "    \"\"\"\n",
    "    Identify boundary points using a Voronoi-based approach.\n",
    "    \"\"\"\n",
    "    # Train a KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    boundary_points = []\n",
    "    boundary_labels = []\n",
    "    \n",
    "    for i, point in enumerate(X_train):\n",
    "        # Predict the label of the point using its neighbors\n",
    "        neighbors = knn.kneighbors([point], return_distance=False)[0]\n",
    "        neighbor_labels = y_train[neighbors]\n",
    "        \n",
    "        # Check if neighbors belong to more than one class\n",
    "        if len(np.unique(neighbor_labels)) > 1:\n",
    "            boundary_points.append(point)\n",
    "            boundary_labels.append(y_train[i])\n",
    "    \n",
    "    return np.array(boundary_points), np.array(boundary_labels)\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply VBNN\n",
    "k = 3\n",
    "boundary_X_train, boundary_y_train = find_voronoi_boundary_points(X_train, y_train, k=k)\n",
    "\n",
    "# Evaluate using KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(boundary_X_train, boundary_y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Results\n",
    "print(f\"Original training size: {len(X_train)}\")\n",
    "print(f\"Boundary training size: {len(boundary_X_train)}\")\n",
    "print(f\"Accuracy with boundary dataset: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305d3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
